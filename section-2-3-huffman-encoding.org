#+title: 2.3 Example: Huffman Encoding

* Exercise 2.67
Define a encoding tree and a sample message:

#+begin_src scheme :eval never
(define sample-tree
  (make-code-tree
   (make-leaf 'A 4)
   (make-code-tree
    (make-leaf 'B 2)
    (make-code-tree
     (make-leaf 'D 1)
     (make-leaf 'C 1)))))

(define sample-message
  '(0 1 1 0 0 1 0 1 0 1 1 1 0))
#+end_src

Use the decode procedure to decode the message, and give the result.

** Solution
Tree graph:

#+INCLUDE: "annex/2_68-tree-graph.dot" src dot :file annex/2_68-tree-graph.svg :cmdline -Tsvg

Decoding result:
#+begin_example
$1 = (A D A B B C A)
#+end_example

* TODO Exercise 2.68
The encode procedure takes as arguments a message and a tree and produces the list of bits that gives the encoded message.

#+begin_src scheme :eval never
(define (encode message tree)
  (if (null? message)
      '()
      (append
       (encode-symbol (car message)
                      tree)
       (encode (cdr message) tree))))
#+end_src

Encode-symbol is a procedure, which you must write, that returns the list of bits that encodes a given symbol according to a given tree.  You should design encode-symbol so that it signals an error if the symbol is not in the tree at all.  Test your procedure by encoding the result you obtained in Exercise 2.67 with the sample tree and seeing whether it is the same as the original sample message.

* TODO Exercise 2.69
The following procedure takes as its argument a list of symbol-frequency pairs (where no symbol appears in more than one pair) and generates a Huffman encoding tree according to the Huffman algorithm.

#+begin_src scheme :eval never
(define (generate-huffman-tree pairs)
  (successive-merge
   (make-leaf-set pairs)))
#+end_src

Make-leaf-set is the procedure given above that transforms the list of pairs into an ordered set of leaves.  Successive-merge is the procedure you must write, using make-code-tree to successively merge the smallest-weight elements of the set until there is only one element left, which is the desired Huffman tree.  (This procedure is slightly tricky, but not really complicated.  If you find yourself designing a complex procedure, then you are almost certainly doing something wrong.  You can take significant advantage of the fact that we are using an ordered set representation.)

* TODO Exercise 2.70
The following eight-symbol alphabet with associated relative frequencies was designed to efficiently encode the lyrics of 1950s rock songs.  (Note that the “symbols” of an “alphabet” need not be individual letters.)

#+begin_example
A    2    NA  16
BOOM 1    SHA  3
GET  2    YIP  9
JOB  2    WAH  1
#+end_example

Use generate-huffman-tree (Exercise 2.69) to generate a corresponding Huffman tree, and use encode (Exercise 2.68) to encode the following message:

#+begin_verse
Get a job
Sha na na na na na na na na

Get a job
Sha na na na na na na na na

Wah yip yip yip yip
yip yip yip yip yip
Sha boom
#+end_verse

How many bits are required for the encoding?  What is the smallest number of bits that would be needed to encode this song if we used a fixed-length code for the eight-symbol alphabet?

* TODO Exercise 2.71
Suppose we have a Huffman tree for an alphabet of $n$ symbols, and that the relative frequencies of the symbols are $1, 2, 4, \cdots, 2^{n−1}$.  Sketch the tree for $n = 5$; for $n = 10$.  In such a tree (for general $n$) how many bits are required to encode the most frequent symbol?  The least frequent symbol?

* TODO Exercise 2.72
Consider the encoding procedure that you designed in Exercise 2.68.  What is the order of growth in the number of steps needed to encode a symbol?  Be sure to include the number of steps needed to search the symbol list at each node encountered.  To answer this question in general is difficult.  Consider the special case where the relative frequencies of the $n$ symbols are as described in Exercise 2.71,  and give the order of growth (as a function of $n$) of the number of steps needed to encode the most frequent and least frequent symbols in the alphabet.
